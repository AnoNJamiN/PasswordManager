{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BHASneQ8ra8p"
      },
      "outputs": [],
      "source": [
        "## Machine Translation Seq2Seq"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "if torch.cuda.is_available():\n",
        "    device=torch.device(type='cuda', index=0)\n",
        "else:\n",
        "    device=torch.device(type='cpu', index=0)\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QfXDPdgnr1Df",
        "outputId": "a67ed878-4147-4081-fe59-9c6fae2a0e48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Major Steps**\n",
        "\n",
        "\n",
        "*   Data Downloading\n",
        "*   Data Loading(Custom Dataset,vocab)\n",
        "*   Model Defination\n",
        "*   Traning testing\n",
        "*   Prediction  \n"
      ],
      "metadata": {
        "id": "wrHSmkaZv7ju"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "!pip install --upgrade torch torchtext"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fjM8dcqVxA0l",
        "outputId": "e1e3a223-74ad-4c9c-9da6-55354714adba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: torchtext in /usr/local/lib/python3.12/dist-packages (0.18.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from torchtext) (4.67.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from torchtext) (2.32.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchtext) (2.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->torchtext) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->torchtext) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->torchtext) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->torchtext) (2025.10.5)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/base_command.py\", line 179, in exc_logging_wrapper\n",
            "    status = run_func(*args)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/req_command.py\", line 67, in wrapper\n",
            "    return func(self, options, args)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/commands/install.py\", line 447, in run\n",
            "    conflicts = self._determine_conflicts(to_install)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/commands/install.py\", line 578, in _determine_conflicts\n",
            "    return check_install_conflicts(to_install)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/operations/check.py\", line 101, in check_install_conflicts\n",
            "    package_set, _ = create_package_set_from_installed()\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/operations/check.py\", line 39, in create_package_set_from_installed\n",
            "    for dist in env.iter_installed_distributions(local_only=False, skip=()):\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/metadata/base.py\", line 664, in <genexpr>\n",
            "    return (d for d in it if d.canonical_name not in skip)\n",
            "                       ^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/metadata/base.py\", line 612, in iter_all_distributions\n",
            "    for dist in self._iter_distributions():\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/metadata/importlib/_envs.py\", line 176, in _iter_distributions\n",
            "    yield from finder.find(location)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/metadata/importlib/_envs.py\", line 79, in find\n",
            "    for dist, info_location in self._find_impl(location):\n",
            "                               ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/metadata/importlib/_envs.py\", line 61, in _find_impl\n",
            "    for dist in importlib.metadata.distributions(path=[location]):\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/importlib/metadata/__init__.py\", line 347, in __new__\n",
            "    def __new__(cls, *args, **kwargs):\n",
            "\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/pip3\", line 10, in <module>\n",
            "    sys.exit(main())\n",
            "             ^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/main.py\", line 80, in main\n",
            "    return command.main(cmd_args)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/base_command.py\", line 100, in main\n",
            "    return self._main(args)\n",
            "           ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/base_command.py\", line 232, in _main\n",
            "    return run(options, args)\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/base_command.py\", line 215, in exc_logging_wrapper\n",
            "    logger.critical(\"Operation cancelled by user\")\n",
            "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1586, in critical\n",
            "    self._log(CRITICAL, msg, args, **kwargs)\n",
            "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1682, in _log\n",
            "    record = self.makeRecord(self.name, level, fn, lno, msg, args,\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1645, in makeRecord\n",
            "    def makeRecord(self, name, level, fn, lno, msg, args, exc_info,\n",
            "\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Downloading\n",
        "train_url = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/training.tar.gz\"\n",
        "valid_url = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/validation.tar.gz\"\n",
        "\n",
        "print(\"Downloading training data...\")\n",
        "!wget -q --show-progress {train_url}\n",
        "\n",
        "print(\"\\nDownloading validation data...\")\n",
        "!wget -q --show-progress {valid_url}\n",
        "\n",
        "print(\"\\nExtracting files...\")\n",
        "!tar -xzf training.tar.gz\n",
        "!tar -xzf validation.tar.gz\n",
        "\n",
        "print(\"\\nExtraction complete. Resulting text files:\")\n",
        "!ls -l *.de *.en\n",
        "# data to df\n",
        "import pandas as pd\n",
        "def create_translation_dataframe(german_filepath, english_filepath):\n",
        "    try:\n",
        "        with open(german_filepath, 'r', encoding='utf-8') as f:\n",
        "            german_lines = [line.strip() for line in f.readlines()]\n",
        "\n",
        "        with open(english_filepath, 'r', encoding='utf-8') as f:\n",
        "            english_lines = [line.strip() for line in f.readlines()]\n",
        "\n",
        "        if len(german_lines) != len(english_lines):\n",
        "            print(\"Error: The number of lines in the two files does not match.\")\n",
        "            return None\n",
        "\n",
        "        df = pd.DataFrame({\n",
        "            'german': german_lines,\n",
        "            'english': english_lines\n",
        "        })\n",
        "\n",
        "        return df\n",
        "\n",
        "    except FileNotFoundError as e:\n",
        "        print(f\"Error: Could not find a file. Please check the path: {e.filename}\")\n",
        "        return None\n",
        "\n",
        "train_df=create_translation_dataframe('train.de','train.en')\n",
        "valid_df=create_translation_dataframe('val.de','val.en')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MI4TVfc0r4bB",
        "outputId": "b3c0d7ed-c5a5-47a4-933f-f0ac9c482862"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading training data...\n",
            "\rtraining.tar.gz.2     0%[                    ]       0  --.-KB/s               \rtraining.tar.gz.2   100%[===================>]   1.15M  --.-KB/s    in 0.04s   \n",
            "\n",
            "Downloading validation data...\n",
            "validation.tar.gz.2 100%[===================>]  45.24K  --.-KB/s    in 0.007s  \n",
            "\n",
            "Extracting files...\n",
            "\n",
            "Extraction complete. Resulting text files:\n",
            "-rw-rw-r-- 1 1000 1000 2110399 Feb  2  2016 train.de\n",
            "-rw-rw-r-- 1 1000 1000 1801239 Feb  2  2016 train.en\n",
            "-rw-rw-r-- 1 1000 1000   75920 Feb  2  2016 val.de\n",
            "-rw-rw-r-- 1 1000 1000   63298 Feb  2  2016 val.en\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download de_core_news_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JZqJuEG5w5u2",
        "outputId": "ab88fb12-ec51-49d7-90e5-cf56abe935f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m138.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Collecting de-core-news-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.8.0/de_core_news_sm-3.8.0-py3-none-any.whl (14.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m136.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: de-core-news-sm\n",
            "Successfully installed de-core-news-sm-3.8.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('de_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "de_nlp = spacy.load('de_core_news_sm')\n",
        "en_nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "def de_tokenizer(text):\n",
        "    return [token.text for token in de_nlp.tokenizer(text)]\n",
        "\n",
        "def en_tokenizer(text):\n",
        "    return [token.text for token in en_nlp.tokenizer(text)]\n",
        "en_tokenizer(train_df['english'][7])\n",
        "\n",
        "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
        "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
        "\n",
        "class Vocab:\n",
        "    def __init__(self, tokenizer):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.word2index = {'<unk>': 0, '<pad>': 1, '<bos>': 2, '<eos>': 3}\n",
        "        self.index2word = {0: '<unk>', 1: '<pad>', 2: '<bos>', 3: '<eos>'}\n",
        "        self.word2count = {}\n",
        "        self.n_words = 4\n",
        "\n",
        "    def build_vocab(self, sentence):\n",
        "        for word in self.tokenizer(sentence):\n",
        "            if word not in self.word2index:\n",
        "                self.word2index[word] = self.n_words\n",
        "                self.index2word[self.n_words] = word\n",
        "                self.word2count[word] = 1\n",
        "                self.n_words += 1\n",
        "            else:\n",
        "                self.word2count[word] += 1\n",
        "vocab_de = Vocab(de_tokenizer)\n",
        "\n",
        "for sentence in train_df['german']:\n",
        "    vocab_de.build_vocab(sentence)\n",
        "\n",
        "print(f\"Built a German vocabulary with {vocab_de.n_words} words.\")\n",
        "\n",
        "vocab_en = Vocab(en_tokenizer)\n",
        "\n",
        "for sentence in train_df['english']:\n",
        "    vocab_en.build_vocab(sentence)\n",
        "\n",
        "print(f\"Built a English vocabulary with {vocab_en.n_words} words.\")#"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6BChdAfH0CzK",
        "outputId": "5b527163-8e3b-4c00-f8d3-95047054898d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Built a German vocabulary with 19214 words.\n",
            "Built a English vocabulary with 10837 words.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, src_sentences, tgt_sentences, vocab_de, vocab_en):\n",
        "        self.src_sentences = src_sentences\n",
        "        self.tgt_sentences = tgt_sentences\n",
        "        self.vocab_de = vocab_de\n",
        "        self.vocab_en = vocab_en\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.src_sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        src_text = self.src_sentences[idx]\n",
        "        tgt_text = self.tgt_sentences[idx]\n",
        "\n",
        "        src_tokens = self.vocab_de.tokenizer(src_text)\n",
        "        tgt_tokens = self.vocab_en.tokenizer(tgt_text)\n",
        "\n",
        "        src_ids = [self.vocab_de.word2index.get(token, 0) for token in src_tokens]\n",
        "        tgt_ids = [self.vocab_en.word2index.get(token, 0) for token in tgt_tokens]\n",
        "\n",
        "        src_ids.append(self.vocab_de.word2index['<eos>'])\n",
        "        tgt_ids.insert(0, self.vocab_en.word2index['<bos>'])\n",
        "        tgt_ids.append(self.vocab_en.word2index['<eos>'])\n",
        "\n",
        "        return torch.tensor(src_ids, dtype=torch.long), torch.tensor(tgt_ids, dtype=torch.long)"
      ],
      "metadata": {
        "id": "I_DjtDTv0FRI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "PAD_IDX = vocab_de.word2index['<pad>']\n",
        "\n",
        "def collate_fn(batch):\n",
        "    src_batch, tgt_batch = [], []\n",
        "    for src_sample, tgt_sample in batch:\n",
        "        src_batch.append(src_sample)\n",
        "        tgt_batch.append(tgt_sample)\n",
        "\n",
        "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX, batch_first=True)\n",
        "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX, batch_first=True)\n",
        "    return src_batch, tgt_batch\n",
        "\n",
        "train_dataset = TranslationDataset(train_df['german'].tolist(), train_df['english'].tolist(), vocab_de, vocab_en)\n",
        "valid_dataset = TranslationDataset(valid_df['german'].tolist(), valid_df['english'].tolist(), vocab_de, vocab_en)\n",
        "\n",
        "batch_size = 32\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)"
      ],
      "metadata": {
        "id": "ZRI7pIrU5yYa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_size, embed_size, hidden_size, dropout_p=0.1):\n",
        "        super().__init__()\n",
        "        self.e = nn.Embedding(input_size, embed_size)\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "        self.gru = nn.GRU(embed_size, hidden_size, batch_first=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.e(x)\n",
        "        x = self.dropout(x)\n",
        "        outputs, hidden = self.gru(x)\n",
        "        return outputs, hidden\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_size, embed_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.e = nn.Embedding(output_size, embed_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.gru = nn.GRU(embed_size, hidden_size, batch_first=True)\n",
        "        self.lin = nn.Linear(hidden_size, output_size)\n",
        "        self.lsoftmax = nn.LogSoftmax(dim=-1)\n",
        "\n",
        "    def forward(self, x, prev_hidden):\n",
        "        x = self.e(x)\n",
        "        x = self.relu(x)\n",
        "        output, hidden = self.gru(x, prev_hidden)\n",
        "        y = self.lin(output)\n",
        "        y = self.lsoftmax(y)\n",
        "        return y, hidden"
      ],
      "metadata": {
        "id": "gv3RNAAP6Tq3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(encoder, decoder, opt_e, opt_d, loss_fn):\n",
        "    encoder.train()\n",
        "    decoder.train()\n",
        "    track_loss = 0\n",
        "\n",
        "    for i, (s_ids, t_ids) in enumerate(train_dataloader):\n",
        "        s_ids = s_ids.to(device)\n",
        "        t_ids = t_ids.to(device)\n",
        "\n",
        "        encoder_outputs, encoder_hidden = encoder(s_ids)\n",
        "        decoder_hidden = encoder_hidden\n",
        "\n",
        "        yhats, decoder_hidden = decoder(t_ids[:, :-1], decoder_hidden)\n",
        "\n",
        "        gt = t_ids[:, 1:]\n",
        "        yhats_reshaped = yhats.view(-1, yhats.shape[-1])\n",
        "        gt_reshaped = gt.reshape(-1)\n",
        "\n",
        "        loss = loss_fn(yhats_reshaped, gt_reshaped)\n",
        "        track_loss += loss.item()\n",
        "\n",
        "        opt_e.zero_grad()\n",
        "        opt_d.zero_grad()\n",
        "        loss.backward()\n",
        "        opt_e.step()\n",
        "        opt_d.step()\n",
        "\n",
        "    return track_loss / (i + 1)\n",
        "\n",
        "def eval_one_epoch(encoder, decoder, loss_fn):\n",
        "    encoder.eval()\n",
        "    decoder.eval()\n",
        "    track_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (s_ids, t_ids) in enumerate(valid_dataloader):\n",
        "            s_ids = s_ids.to(device)\n",
        "            t_ids = t_ids.to(device)\n",
        "\n",
        "            encoder_outputs, encoder_hidden = encoder(s_ids)\n",
        "            decoder_hidden = encoder_hidden\n",
        "\n",
        "            yhats, decoder_hidden = decoder(t_ids[:, :-1], decoder_hidden)\n",
        "\n",
        "            gt = t_ids[:, 1:]\n",
        "            yhats_reshaped = yhats.view(-1, yhats.shape[-1])\n",
        "            gt_reshaped = gt.reshape(-1)\n",
        "\n",
        "            loss = loss_fn(yhats_reshaped, gt_reshaped)\n",
        "            track_loss += loss.item()\n",
        "\n",
        "    return track_loss / (i + 1)"
      ],
      "metadata": {
        "id": "H9VfESfS6UGt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "embed_size = 300\n",
        "hidden_size = 512\n",
        "\n",
        "encoder = Encoder(vocab_de.n_words, embed_size, hidden_size).to(device)\n",
        "decoder = Decoder(vocab_en.n_words, embed_size, hidden_size).to(device)\n",
        "\n",
        "loss_fn = nn.NLLLoss(ignore_index=PAD_IDX).to(device)\n",
        "lr = 0.001\n",
        "opt_e = optim.Adam(params=encoder.parameters(), lr=lr)\n",
        "opt_d = optim.Adam(params=decoder.parameters(), lr=lr)\n",
        "\n",
        "n_epochs = 10\n",
        "\n",
        "for e in range(n_epochs):\n",
        "    train_loss = train_one_epoch(encoder, decoder, opt_e, opt_d, loss_fn)\n",
        "    eval_loss = eval_one_epoch(encoder, decoder, loss_fn)\n",
        "    print(f\"Epoch {e+1}/{n_epochs}, Train Loss: {train_loss:.4f}, Eval Loss: {eval_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G9LUufHG6WRS",
        "outputId": "553a526b-ebd6-4a20-d0b5-38fa26e61706"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Train Loss: 3.8132, Eval Loss: 3.1398\n",
            "Epoch 2/10, Train Loss: 2.7891, Eval Loss: 2.7676\n",
            "Epoch 3/10, Train Loss: 2.2604, Eval Loss: 2.6332\n",
            "Epoch 4/10, Train Loss: 1.8480, Eval Loss: 2.6013\n",
            "Epoch 5/10, Train Loss: 1.5166, Eval Loss: 2.6123\n",
            "Epoch 6/10, Train Loss: 1.2530, Eval Loss: 2.6496\n",
            "Epoch 7/10, Train Loss: 1.0446, Eval Loss: 2.7120\n",
            "Epoch 8/10, Train Loss: 0.8761, Eval Loss: 2.8066\n",
            "Epoch 9/10, Train Loss: 0.7449, Eval Loss: 2.8886\n",
            "Epoch 10/10, Train Loss: 0.6338, Eval Loss: 2.9766\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def translate_sentence(german_sentence, encoder, decoder, vocab_de, vocab_en, device, max_length=50):\n",
        "    encoder.eval()\n",
        "    decoder.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        src_tokens = vocab_de.tokenizer(german_sentence)\n",
        "        src_ids = [vocab_de.word2index.get(token, 0) for token in src_tokens]\n",
        "        src_ids.append(vocab_de.word2index['<eos>'])\n",
        "\n",
        "        src_tensor = torch.tensor(src_ids, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "        encoder_outputs, encoder_hidden = encoder(src_tensor)\n",
        "        decoder_hidden = encoder_hidden\n",
        "\n",
        "        decoder_input = torch.tensor([vocab_en.word2index['<bos>']], device=device)\n",
        "\n",
        "        translated_ids = []\n",
        "\n",
        "        for _ in range(max_length):\n",
        "            probs, decoder_hidden = decoder(decoder_input.unsqueeze(1), decoder_hidden)\n",
        "            _, top_id = torch.topk(probs, 1, dim=-1)\n",
        "            predicted_id = top_id.squeeze().item()\n",
        "\n",
        "            if predicted_id == vocab_en.word2index['<eos>']:\n",
        "                break\n",
        "\n",
        "            translated_ids.append(predicted_id)\n",
        "            decoder_input = torch.tensor([predicted_id], device=device)\n",
        "\n",
        "        translated_tokens = [vocab_en.index2word.get(idx, '<unk>') for idx in translated_ids]\n",
        "        return \" \".join(translated_tokens)\n",
        "test_sentence = \"Eine Gruppe von Männern lädt Baumwolle auf einen Lastwagen.\"\n",
        "\n",
        "translation = translate_sentence(test_sentence, encoder, decoder, vocab_de, vocab_en, device)\n",
        "\n",
        "print(f\"Original (de): {test_sentence}\")\n",
        "print(f\"Translated (en): {translation}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qOrj2Lx96YpL",
        "outputId": "f72d3b5d-b2cb-4af1-afcb-6f1c89bd0cc3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original (de): Eine Gruppe von Männern lädt Baumwolle auf einen Lastwagen.\n",
            "Translated (en): A group of men are organizing a child on a truck .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aVQufOLb7THh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}