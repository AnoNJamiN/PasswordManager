{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1sqYHuI5vilb",
        "outputId": "00bd0aaf-38ba-4abe-9a6e-2fddd3d03569"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda:0\n",
            "Downloading and extracting data...\n",
            "Data download complete.\n"
          ]
        }
      ],
      "source": [
        "# --- Core Imports ---\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "import spacy\n",
        "import random\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(type='cuda', index=0)\n",
        "else:\n",
        "    device = torch.device(type='cpu', index=0)\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# --- Data Downloading and Loading ---\n",
        "print(\"Downloading and extracting data...\")\n",
        "!wget -q -O training.tar.gz \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/training.tar.gz\"\n",
        "!wget -q -O validation.tar.gz \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/validation.tar.gz\"\n",
        "!tar -xzf training.tar.gz\n",
        "!tar -xzf validation.tar.gz\n",
        "print(\"Data download complete.\")\n",
        "\n",
        "def create_translation_dataframe(german_filepath, english_filepath):\n",
        "    with open(german_filepath, 'r', encoding='utf-8') as f:\n",
        "        german_lines = [line.strip() for line in f.readlines()]\n",
        "    with open(english_filepath, 'r', encoding='utf-8') as f:\n",
        "        english_lines = [line.strip() for line in f.readlines()]\n",
        "    return pd.DataFrame({'german': german_lines, 'english': english_lines})\n",
        "\n",
        "train_df = create_translation_dataframe('train.de', 'train.en')\n",
        "valid_df = create_translation_dataframe('val.de', 'val.en')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Dhiw0y4vio_",
        "outputId": "4e1d594c-2209-40cd-aad9-2d58cf799b43"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Setting up tokenizers and building vocabularies...\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m134.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('de_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Built a German vocabulary with 19214 words.\n",
            "Built an English vocabulary with 10837 words.\n"
          ]
        }
      ],
      "source": [
        "# ---Tokenization and Vocabulary Building ---\n",
        "print(\"Setting up tokenizers and building vocabularies...\")\n",
        "!python -m spacy download en_core_web_sm -q\n",
        "!python -m spacy download de_core_news_sm -q\n",
        "\n",
        "de_nlp = spacy.load('de_core_news_sm')\n",
        "en_nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "def de_tokenizer(text):\n",
        "    return [token.text for token in de_nlp.tokenizer(text)]\n",
        "\n",
        "def en_tokenizer(text):\n",
        "    return [token.text for token in en_nlp.tokenizer(text)]\n",
        "\n",
        "class Vocab:\n",
        "    def __init__(self, tokenizer):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.word2index = {'<unk>': 0, '<pad>': 1, '<bos>': 2, '<eos>': 3}\n",
        "        self.index2word = {0: '<unk>', 1: '<pad>', 2: '<bos>', 3: '<eos>'}\n",
        "        self.n_words = 4\n",
        "\n",
        "    def build_vocab(self, sentence_list):\n",
        "        for sentence in sentence_list:\n",
        "            for word in self.tokenizer(sentence):\n",
        "                if word not in self.word2index:\n",
        "                    self.word2index[word] = self.n_words\n",
        "                    self.index2word[self.n_words] = word\n",
        "                    self.n_words += 1\n",
        "\n",
        "vocab_de = Vocab(de_tokenizer)\n",
        "vocab_de.build_vocab(train_df['german'])\n",
        "print(f\"Built a German vocabulary with {vocab_de.n_words} words.\")\n",
        "\n",
        "vocab_en = Vocab(en_tokenizer)\n",
        "vocab_en.build_vocab(train_df['english'])\n",
        "print(f\"Built an English vocabulary with {vocab_en.n_words} words.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WGzHREv7vitC"
      },
      "outputs": [],
      "source": [
        "# ---Custom Dataset for DataLoader ---\n",
        "\n",
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, src_sentences, tgt_sentences, vocab_de, vocab_en):\n",
        "        self.src_sentences = src_sentences\n",
        "        self.tgt_sentences = tgt_sentences\n",
        "        self.vocab_de = vocab_de\n",
        "        self.vocab_en = vocab_en\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.src_sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        src_text = self.src_sentences[idx]\n",
        "        tgt_text = self.tgt_sentences[idx]\n",
        "        src_ids = [self.vocab_de.word2index.get(token, 0) for token in self.vocab_de.tokenizer(src_text)]\n",
        "        tgt_ids = [self.vocab_en.word2index.get(token, 0) for token in self.vocab_en.tokenizer(tgt_text)]\n",
        "        src_ids.append(self.vocab_de.word2index['<eos>'])\n",
        "        tgt_ids.insert(0, self.vocab_en.word2index['<bos>'])\n",
        "        tgt_ids.append(self.vocab_en.word2index['<eos>'])\n",
        "        return torch.tensor(src_ids, dtype=torch.long), torch.tensor(tgt_ids, dtype=torch.long)\n",
        "\n",
        "PAD_IDX = vocab_de.word2index['<pad>']\n",
        "def collate_fn(batch):\n",
        "    src_batch, tgt_batch = [], []\n",
        "    for src_sample, tgt_sample in batch:\n",
        "        src_batch.append(src_sample)\n",
        "        tgt_batch.append(tgt_sample)\n",
        "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX, batch_first=True)\n",
        "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX, batch_first=True)\n",
        "    return src_batch, tgt_batch\n",
        "\n",
        "train_dataset = TranslationDataset(train_df['german'].tolist(), train_df['english'].tolist(), vocab_de, vocab_en)\n",
        "valid_dataset = TranslationDataset(valid_df['german'].tolist(), valid_df['english'].tolist(), vocab_de, vocab_en)\n",
        "\n",
        "batch_size = 32\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4nhy8AeiviwF"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_size, embed_size, hidden_size, dropout_p=0.1):\n",
        "        super().__init__()\n",
        "        self.e = nn.Embedding(input_size, embed_size)\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "        # Using a unidirectional GRU as requested\n",
        "        self.gru = nn.GRU(embed_size, hidden_size, batch_first=True)\n",
        "\n",
        "    def forward(self, x, lengths):\n",
        "        x = self.e(x)\n",
        "        x = self.dropout(x)\n",
        "        # Pack sequence to handle padding correctly\n",
        "        x = pack_padded_sequence(x, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
        "        outputs, hidden = self.gru(x)\n",
        "        # Unpack sequence\n",
        "        outputs, _ = pad_packed_sequence(outputs, batch_first=True)\n",
        "        return outputs, hidden\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_size, embed_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.e = nn.Embedding(output_size, embed_size)\n",
        "        self.dropout = nn.Dropout()\n",
        "        # Decoder GRU input is the concatenation of the embedded word and the context vector\n",
        "        self.gru = nn.GRU(embed_size + hidden_size, hidden_size, batch_first=True)\n",
        "        self.lin = nn.Linear(hidden_size, output_size)\n",
        "        self.lsoftmax = nn.LogSoftmax(dim=-1)\n",
        "\n",
        "    def forward(self, x, context, prev_hidden):\n",
        "        x = self.e(x) # [batch_size, 1, embed_size]\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # Concatenate the embedded input and the context vector\n",
        "        input_gru = torch.cat((x, context), dim=2) # [batch_size, 1, embed_size + hidden_size]\n",
        "\n",
        "        # Pass through GRU\n",
        "        output, hidden = self.gru(input_gru, prev_hidden)\n",
        "\n",
        "        # Get prediction\n",
        "        y = self.lin(output) # [batch_size, 1, output_size]\n",
        "        y = self.lsoftmax(y) # Apply LogSoftmax for NLLLoss\n",
        "        return y, hidden"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ibkBOvmvviys"
      },
      "outputs": [],
      "source": [
        "# ---Training-evaluation ---\n",
        "\n",
        "def train_one_epoch(encoder, decoder, opt_e, opt_d, loss_fn, dataloader):\n",
        "    encoder.train()\n",
        "    decoder.train()\n",
        "    track_loss = 0\n",
        "\n",
        "    for i, (s_ids, t_ids) in enumerate(dataloader):\n",
        "        s_ids = s_ids.to(device)\n",
        "        t_ids = t_ids.to(device)\n",
        "\n",
        "        src_lengths = torch.sum(s_ids != PAD_IDX, dim=1)\n",
        "\n",
        "        opt_e.zero_grad()\n",
        "        opt_d.zero_grad()\n",
        "\n",
        "        _, encoder_hidden = encoder(s_ids, src_lengths)\n",
        "\n",
        "        decoder_hidden = encoder_hidden\n",
        "        decoder_input = t_ids[:, 0].unsqueeze(1)\n",
        "\n",
        "        context = encoder_hidden.permute(1, 0, 2)\n",
        "\n",
        "        batch_loss = 0\n",
        "        for t in range(1, t_ids.shape[1]):\n",
        "            yhats, decoder_hidden = decoder(decoder_input, context, decoder_hidden)\n",
        "\n",
        "            decoder_input = t_ids[:, t].unsqueeze(1)\n",
        "            batch_loss += loss_fn(yhats.squeeze(1), t_ids[:, t])\n",
        "\n",
        "        batch_loss.backward()\n",
        "        opt_e.step()\n",
        "        opt_d.step()\n",
        "\n",
        "        track_loss += batch_loss.item() / (t_ids.shape[1] - 1)\n",
        "\n",
        "    return track_loss / len(dataloader)\n",
        "\n",
        "def eval_one_epoch(encoder, decoder, loss_fn, dataloader):\n",
        "    encoder.eval()\n",
        "    decoder.eval()\n",
        "    track_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (s_ids, t_ids) in enumerate(dataloader):\n",
        "            s_ids = s_ids.to(device)\n",
        "            t_ids = t_ids.to(device)\n",
        "\n",
        "            src_lengths = torch.sum(s_ids != PAD_IDX, dim=1)\n",
        "\n",
        "            _, encoder_hidden = encoder(s_ids, src_lengths)\n",
        "\n",
        "            decoder_hidden = encoder_hidden\n",
        "            decoder_input = t_ids[:, 0].unsqueeze(1)\n",
        "\n",
        "            context = encoder_hidden.permute(1, 0, 2)\n",
        "\n",
        "            batch_loss = 0\n",
        "            for t in range(1, t_ids.shape[1]):\n",
        "                yhats, decoder_hidden = decoder(decoder_input, context, decoder_hidden)\n",
        "                decoder_input = t_ids[:, t].unsqueeze(1)\n",
        "                batch_loss += loss_fn(yhats.squeeze(1), t_ids[:, t])\n",
        "\n",
        "            track_loss += batch_loss.item() / (t_ids.shape[1] - 1)\n",
        "\n",
        "    return track_loss / len(dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "1XHBeePwvi05",
        "outputId": "3303b28c-03fd-41d6-a420-d7a4eab16fd3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Starting training...\n",
            "Epoch 1/10, Train Loss: 3.7504, Eval Loss: 3.2127\n",
            "Epoch 2/10, Train Loss: 2.7633, Eval Loss: 2.8713\n",
            "Epoch 3/10, Train Loss: 2.2208, Eval Loss: 2.7555\n",
            "Epoch 4/10, Train Loss: 1.8398, Eval Loss: 2.7047\n",
            "Epoch 5/10, Train Loss: 1.5814, Eval Loss: 2.6927\n",
            "Epoch 6/10, Train Loss: 1.4270, Eval Loss: 2.7151\n",
            "Epoch 7/10, Train Loss: 1.3114, Eval Loss: 2.7132\n",
            "Epoch 8/10, Train Loss: 1.2064, Eval Loss: 2.7663\n",
            "Epoch 9/10, Train Loss: 1.1309, Eval Loss: 2.7766\n",
            "Epoch 10/10, Train Loss: 1.0579, Eval Loss: 2.8423\n"
          ]
        }
      ],
      "source": [
        "# ---Train Test main function ---\n",
        "\n",
        "embed_size = 300\n",
        "hidden_size = 512\n",
        "\n",
        "encoder = Encoder(vocab_de.n_words, embed_size, hidden_size).to(device)\n",
        "decoder = Decoder(vocab_en.n_words, embed_size, hidden_size).to(device)\n",
        "\n",
        "loss_fn = nn.NLLLoss(ignore_index=PAD_IDX).to(device)\n",
        "lr = 0.001\n",
        "opt_e = optim.Adam(params=encoder.parameters(), lr=lr)\n",
        "opt_d = optim.Adam(params=decoder.parameters(), lr=lr)\n",
        "\n",
        "n_epochs = 10\n",
        "print(\"\\nStarting training...\")\n",
        "for e in range(n_epochs):\n",
        "    train_loss = train_one_epoch(encoder, decoder, opt_e, opt_d, loss_fn, train_dataloader)\n",
        "    eval_loss = eval_one_epoch(encoder, decoder, loss_fn, valid_dataloader)\n",
        "    print(f\"Epoch {e+1}/{n_epochs}, Train Loss: {train_loss:.4f}, Eval Loss: {eval_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "KM8mCQkPu837",
        "outputId": "4fa1e1fa-6998-47c2-ebb2-86a4bd234e27"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "--- Random Test from Validation Set ---\n",
            "Original (de):         Eine junge Frau und eine ältere Frau in traditionellen Saris spinnen Textilien, während drei weitere Personen in moderner Kleidung nur von der Taille abwärts auf dem Bild zu sehen sind.\n",
            "Model's Translation (en): A young woman and a young woman are lined up from various , as they walk in a public place , as they walk by different colored , as they walk by different colored , - style outfits . <eos>\n",
            "Actual Translation (en):  A young woman and older woman wear traditional saris as they spin textiles, three people are pictured at only the waists, and wear modern clothes.\n",
            "\n",
            "\n",
            "--- Static User-Provided Test ---\n",
            "Original (de):         Wir schlagen eine neue einfache Netzwerkarchitektur vor, den Transformer, der ausschließlich auf Aufmerksamkeitsmechanismen basiert und vollständig auf Wiederholungen und Faltungen verzichtet.\n",
            "Model's Translation (en): Pitcher on a stage , a man in a spiked , and a spiked , working , and accordion for the puck . <eos>\n",
            "Actual Translation (en):  We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.\n"
          ]
        }
      ],
      "source": [
        "# ---Final Test(convetts tensors to token id to words) ---\n",
        "def translate_sentence(german_sentence, encoder, decoder, vocab_de, vocab_en, device, max_length=50):\n",
        "    encoder.eval()\n",
        "    decoder.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        src_tokens = vocab_de.tokenizer(german_sentence)\n",
        "        src_ids = [vocab_de.word2index.get(token, 0) for token in src_tokens]\n",
        "        src_ids.append(vocab_de.word2index['<eos>'])\n",
        "        src_tensor = torch.tensor(src_ids, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "        src_length = torch.tensor([len(src_ids)])\n",
        "\n",
        "        _, encoder_hidden = encoder(src_tensor, src_length)\n",
        "        context = encoder_hidden.permute(1, 0, 2)\n",
        "\n",
        "        decoder_hidden = encoder_hidden\n",
        "        decoder_input = torch.tensor([[vocab_en.word2index['<bos>']]], device=device)\n",
        "\n",
        "        translated_ids = []\n",
        "        for _ in range(max_length):\n",
        "            yhats, decoder_hidden = decoder(decoder_input, context, decoder_hidden)\n",
        "\n",
        "            predicted_id = yhats.squeeze(1).argmax(dim=1).item()\n",
        "            translated_ids.append(predicted_id)\n",
        "\n",
        "            if predicted_id == vocab_en.word2index['<eos>']:\n",
        "                break\n",
        "\n",
        "            decoder_input = torch.tensor([[predicted_id]], device=device)\n",
        "\n",
        "        translated_tokens = [vocab_en.index2word.get(idx, '<unk>') for idx in translated_ids]\n",
        "        return \" \".join(translated_tokens)\n",
        "\n",
        "def run_translation_test(german_sentence, actual_english_sentence):\n",
        "    model_translation = translate_sentence(\n",
        "        german_sentence, encoder, decoder, vocab_de, vocab_en, device\n",
        "    )\n",
        "    print(f\"Original (de):         {german_sentence}\")\n",
        "    print(f\"Model's Translation (en): {model_translation}\")\n",
        "    print(f\"Actual Translation (en):  {actual_english_sentence}\")\n",
        "\n",
        "\n",
        "# --- Test 1: Random Row from the Validation DataFrame ---\n",
        "8print(\"\\n\\n--- Random Test from Validation Set ---\")\n",
        "try:\n",
        "    random_sample = valid_df.sample(n=1)\n",
        "    random_german_sentence = random_sample['german'].iloc[0]\n",
        "    random_english_sentence = random_sample['english'].iloc[0]\n",
        "    run_translation_test(random_german_sentence, random_english_sentence)\n",
        "except Exception as e:\n",
        "    print(f\"Could not perform the random test: {e}\")\n",
        "\n",
        "\n",
        "# --- Test 2: Static, User-Provided Sentence ---\n",
        "print(\"\\n\\n--- Static User-Provided Test ---\")\n",
        "static_german_sentence = \"Wir schlagen eine neue einfache Netzwerkarchitektur vor, den Transformer, der ausschließlich auf Aufmerksamkeitsmechanismen basiert und vollständig auf Wiederholungen und Faltungen verzichtet.\"\n",
        "static_english_translation = \"We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.\"\n",
        "run_translation_test(static_german_sentence, static_english_translation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "70Tv-vcH13yh"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}