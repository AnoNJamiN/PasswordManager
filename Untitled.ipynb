{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e277886a-2ea5-4abf-bc7a-b9a8d6ce599b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.3.1+cu118\n",
      "TorchText version: 0.18.0+cpu\n",
      "CUDA available: True\n",
      "0.9.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "import torchdata\n",
    "\n",
    "#torchtext.disable_torchtext_deprecation_warning()\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"TorchText version:\", torchtext.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "\n",
    "print(torchdata.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05c1867e-bc31-41ef-b10c-b15ea4412f63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jamin Carter\\Desktop\\SEM V\\NLP\\Practicals\\Practical 3\\final final\\torch_env\\Lib\\site-packages\\torchtext\\datasets\\__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "C:\\Users\\Jamin Carter\\Desktop\\SEM V\\NLP\\Practicals\\Practical 3\\final final\\torch_env\\Lib\\site-packages\\torchtext\\data\\__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "C:\\Users\\Jamin Carter\\Desktop\\SEM V\\NLP\\Practicals\\Practical 3\\final final\\torch_env\\Lib\\site-packages\\torchtext\\vocab\\__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "C:\\Users\\Jamin Carter\\Desktop\\SEM V\\NLP\\Practicals\\Practical 3\\final final\\torch_env\\Lib\\site-packages\\torchtext\\utils.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "C:\\Users\\Jamin Carter\\Desktop\\SEM V\\NLP\\Practicals\\Practical 3\\final final\\torch_env\\Lib\\site-packages\\torchdata\\datapipes\\__init__.py:18: UserWarning: \n",
      "################################################################################\n",
      "WARNING!\n",
      "The 'datapipes', 'dataloader2' modules are deprecated and will be removed in a\n",
      "future torchdata release! Please see https://github.com/pytorch/data/issues/1196\n",
      "to learn more and leave feedback.\n",
      "################################################################################\n",
      "\n",
      "  deprecation_warning()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 8\n",
      "Example token IDs: tensor([  10,    8,   15,  223,   37,   93,    2,    5, 2288, 1463])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchtext.datasets import IMDB\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "# --- Step 1: Tokenizer\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "# --- Step 2: Load IMDB dataset\n",
    "train_iter, test_iter = IMDB(split=('train', 'test'))\n",
    "\n",
    "# --- Step 3: Build vocabulary from train data\n",
    "def yield_tokens(data_iter):\n",
    "    for _, text in data_iter:  # ignore labels\n",
    "        yield tokenizer(text)\n",
    "\n",
    "TEXT = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<unk>\"])\n",
    "TEXT.set_default_index(TEXT[\"<unk>\"])\n",
    "\n",
    "# --- Step 4: Reset iterators\n",
    "train_iter, test_iter = IMDB(split=('train', 'test'))\n",
    "\n",
    "# --- Step 5: Prepare data for Word2Vec (token IDs)\n",
    "def collate_batch(batch):\n",
    "    texts = [torch.tensor(TEXT(tokenizer(text)), dtype=torch.long) for _, text in batch]\n",
    "    return texts\n",
    "\n",
    "train_data = list(train_iter)\n",
    "test_data  = list(test_iter)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=8, shuffle=True, collate_fn=collate_batch)\n",
    "test_loader  = torch.utils.data.DataLoader(test_data, batch_size=8, shuffle=False, collate_fn=collate_batch)\n",
    "\n",
    "# --- Step 6: Inspect one batch\n",
    "for texts in train_loader:\n",
    "    print(\"Batch size:\", len(texts))\n",
    "    print(\"Example token IDs:\", texts[0][:10])  # first 10 tokens of first text\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a8521e0-7a79-4b8a-86c6-a76226d29da5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample train review: I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ev ...\n",
      "Sample train label: 1\n",
      "Sample eval review: I love sci-fi and am willing to put up with a lot. Sci-fi movies/TV are usually underfunded, under-appreciated and misunderstood. I tried to like this, I really did, but it is to good TV sci-fi as Bab ...\n",
      "Sample eval label: 1\n"
     ]
    }
   ],
   "source": [
    "# Load IMDB iterators\n",
    "train_iter, test_iter = IMDB(split=('train', 'test'))\n",
    "\n",
    "# Convert iterators to lists\n",
    "train_data = list(train_iter)\n",
    "eval_data  = list(test_iter)\n",
    "\n",
    "# Training data\n",
    "train_texts  = [text for _, text in train_data]\n",
    "train_labels = [label for label, _ in train_data]\n",
    "\n",
    "# Evaluation (test) data\n",
    "eval_texts  = [text for _, text in eval_data]\n",
    "eval_labels = [label for label, _ in eval_data]\n",
    "\n",
    "# Quick check\n",
    "print(\"Sample train review:\", train_texts[0][:200], \"...\")  # print first 200 chars\n",
    "print(\"Sample train label:\", train_labels[0])\n",
    "print(\"Sample eval review:\", eval_texts[0][:200], \"...\")\n",
    "print(\"Sample eval label:\", eval_labels[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "810014f8-def7-441a-ac64-1965c242b12e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 13351\n",
      "Index of <unk>: 0\n",
      "First 20 tokens: ['<unk>', '!', '#1', '#2', '$', '$1', '$10', '$100', '$2', '$3', '$4', '$5', '&', \"'\", '(', ')', '*', '**', '***', '****']\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "min_word_freq = 20  # example threshold\n",
    "\n",
    "# Count word frequencies\n",
    "counter = Counter()\n",
    "for text in train_texts:\n",
    "    counter.update(tokenizer(text))\n",
    "\n",
    "# Keep only tokens that meet frequency threshold\n",
    "filtered_tokens = [tok for tok, freq in counter.items() if freq >= min_word_freq]\n",
    "\n",
    "# Build vocab\n",
    "vocab = build_vocab_from_iterator([filtered_tokens], specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])  # default for unknown tokens\n",
    "\n",
    "# Test\n",
    "# Print vocab info\n",
    "print(\"Vocabulary size:\", len(vocab))\n",
    "unk_index = vocab[\"<unk>\"]\n",
    "print(\"Index of <unk>:\", unk_index)\n",
    "\n",
    "# First 20 tokens\n",
    "print(\"First 20 tokens:\", vocab.get_itos()[:20])  # get_itos() works too\n",
    "# or simply: print(vocab.itos[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4d4f0e6-cdf3-4c2f-b914-e5f1e9ef40b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['this', 'movie', 'was', 'absolutely', 'amazing']\n",
      "IDs: [12012, 7895, 12913, 286, 622]\n"
     ]
    }
   ],
   "source": [
    "sample_review = \"this movie was absolutely amazing\"\n",
    "tokens = tokenizer(sample_review)  # ['this', 'movie', 'was', 'absolutely', 'amazing']\n",
    "\n",
    "# Convert all tokens to IDs at once\n",
    "ids = vocab.lookup_indices(tokens)  # <- expects a list of tokens\n",
    "\n",
    "print(\"Tokens:\", tokens)\n",
    "print(\"IDs:\", ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5766beb-8e38-44df-8adc-45311b64f2bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13351\n"
     ]
    }
   ],
   "source": [
    "vocab_size=vocab.__len__()\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27c4e21d-8264-4907-be91-199d714ff1f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['hello', 'world']\n",
      "Token IDs: [5679, 13190]\n",
      "Type: <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "window_size = 4  # context window size of 9\n",
    "max_seq_len = 256\n",
    "max_norm = 1\n",
    "embed_dim = 300\n",
    "batch_size = 16\n",
    "\n",
    "unk_index = 0  # default index for unknown words\n",
    "\n",
    "# Text pipeline: map tokens → indices\n",
    "text_pipeline = lambda tokens: vocab(tokens)  # vocab is callable now\n",
    "\n",
    "sample_text = \"Hello World\"\n",
    "tokens = tokenizer(sample_text)  # ['Hello', 'World']\n",
    "\n",
    "# Map tokens to IDs\n",
    "sample_ids = text_pipeline(tokens)\n",
    "\n",
    "print(\"Tokens:\", tokens)\n",
    "print(\"Token IDs:\", sample_ids)\n",
    "print(\"Type:\", type(sample_ids))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db77921c-181b-4e90-a5d7-0a6c7945bd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "window_size = 2  # context window\n",
    "max_seq_len = 256\n",
    "unk_index = 0    # index of <unk> in vocab\n",
    "\n",
    "# Text pipeline using new torchtext Vocab API\n",
    "def text_pipeline(text):\n",
    "    tokens = tokenizer(text)\n",
    "    return [vocab[token] if token in vocab else unk_index for token in tokens]\n",
    "\n",
    "# Skip-gram collate function\n",
    "def collate_skipgram(batch, text_pipeline, window_size=4, max_seq_len=256):\n",
    "    batch_input_word, batch_target_words = [], []\n",
    "\n",
    "    for review in batch:\n",
    "        # Convert review (string) → token ids\n",
    "        review_tokens_ids = text_pipeline(review)\n",
    "\n",
    "        # Skip short reviews\n",
    "        if len(review_tokens_ids) < window_size * 2 + 1:\n",
    "            continue\n",
    "\n",
    "        # Truncate if needed\n",
    "        if max_seq_len:\n",
    "            review_tokens_ids = review_tokens_ids[:max_seq_len]\n",
    "\n",
    "        # Sliding window\n",
    "        for idx in range(len(review_tokens_ids)):\n",
    "            input_word = review_tokens_ids[idx]\n",
    "\n",
    "            # Context window boundaries\n",
    "            start = max(idx - window_size, 0)\n",
    "            end   = min(idx + window_size + 1, len(review_tokens_ids))\n",
    "\n",
    "            # Context words = all words in window except the input word itself\n",
    "            context_words = [review_tokens_ids[i] for i in range(start, end) if i != idx]\n",
    "\n",
    "            # Append pairs\n",
    "            batch_input_word.extend([input_word] * len(context_words))\n",
    "            batch_target_words.extend(context_words)\n",
    "\n",
    "    # Convert to tensors\n",
    "    batch_input_word = torch.tensor(batch_input_word, dtype=torch.long)\n",
    "    batch_target_words = torch.tensor(batch_target_words, dtype=torch.long)\n",
    "\n",
    "    return batch_input_word, batch_target_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a493d93b-8624-4770-a2a8-2326629c7107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs shape: torch.Size([52])\n",
      "Targets shape: torch.Size([52])\n",
      "Example input word: 12012\n",
      "Example target word: 7895\n"
     ]
    }
   ],
   "source": [
    "inputs, targets = collate_skipgram(\n",
    "    [\"this movie was absolutely fantastic and i loved it\"],\n",
    "    text_pipeline,\n",
    "    window_size=4\n",
    ")\n",
    "\n",
    "print(\"Inputs shape:\", inputs.shape)\n",
    "print(\"Targets shape:\", targets.shape)\n",
    "\n",
    "if inputs.size(0) > 0:\n",
    "    print(\"Example input word:\", inputs[0].item())\n",
    "    print(\"Example target word:\", targets[0].item())\n",
    "else:\n",
    "    print(\"No training pairs generated\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f9a6bc4-84ab-46d0-aa22-50c2052e2d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# --- SkipGram model\n",
    "class SkipGram(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=300, max_norm=1):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(\n",
    "            num_embeddings=vocab_size,\n",
    "            embedding_dim=embed_dim,\n",
    "            max_norm=max_norm\n",
    "        )\n",
    "        self.linear = nn.Linear(\n",
    "            in_features=embed_dim,\n",
    "            out_features=vocab_size\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x shape: [batch_size]\n",
    "        \"\"\"\n",
    "        x = self.embeddings(x)  # [batch_size, embed_dim]\n",
    "        x = self.linear(x)      # [batch_size, vocab_size]\n",
    "        return x\n",
    "\n",
    "# --- Example: instantiate model\n",
    "vocab_size = len(vocab)  # your vocab object\n",
    "model = SkipGram(vocab_size, embed_dim=300).to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e0ec3511-8dd4-4ffb-ac8e-dcebe181b0e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch on device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --- 1. Convert all text to token IDs\n",
    "mapped_train_data = [text_pipeline(text) for text in train_texts]\n",
    "mapped_eval_data  = [text_pipeline(text) for text in eval_texts]\n",
    "\n",
    "# --- 2. Precompute skip-gram pairs\n",
    "def generate_skipgram_pairs(data, window_size=4):\n",
    "    inputs, targets = [], []\n",
    "    for review_ids in data:\n",
    "        if len(review_ids) < window_size * 2 + 1:\n",
    "            continue\n",
    "        for idx in range(len(review_ids) - window_size * 2):\n",
    "            seq = review_ids[idx: idx + window_size * 2 + 1]\n",
    "            input_word = seq[window_size]\n",
    "            context = seq[:window_size] + seq[window_size+1:]\n",
    "            for target_word in context:\n",
    "                inputs.append(input_word)\n",
    "                targets.append(target_word)\n",
    "    return torch.tensor(inputs, dtype=torch.long), torch.tensor(targets, dtype=torch.long)\n",
    "\n",
    "train_inputs, train_targets = generate_skipgram_pairs(mapped_train_data, window_size=4)\n",
    "eval_inputs, eval_targets   = generate_skipgram_pairs(mapped_eval_data, window_size=4)\n",
    "\n",
    "# --- 3. Create TensorDataset & DataLoader\n",
    "traindl_skipgram = DataLoader(\n",
    "    TensorDataset(train_inputs, train_targets),\n",
    "    batch_size=128,\n",
    "    shuffle=True,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "evaldl_skipgram = DataLoader(\n",
    "    TensorDataset(eval_inputs, eval_targets),\n",
    "    batch_size=128,\n",
    "    shuffle=False,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "# --- 4. Move model to GPU\n",
    "model = SkipGram(len(vocab), embed_dim=300).to(device)\n",
    "\n",
    "# --- 5. Training example\n",
    "for xb, yb in traindl_skipgram:\n",
    "    xb, yb = xb.to(device), yb.to(device)\n",
    "    out = model(xb)\n",
    "    print(\"Batch on device:\", out.device)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eee54ac6-b7e8-4f36-8136-f5b602ec5708",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_pipeline(text):\n",
    "    return vocab(tokenizer(text))\n",
    "\n",
    "def collate_skipgram(batch, text_pipeline, window_size=4, max_seq_len=256):\n",
    "    batch_input_word, batch_target_words = [], []\n",
    "\n",
    "    for _, review in batch: # The batch is a list of (label, text) tuples\n",
    "        review_tokens_ids = text_pipeline(review)\n",
    "        \n",
    "        # Skip short reviews\n",
    "        if len(review_tokens_ids) < window_size * 2 + 1:\n",
    "            continue\n",
    "            \n",
    "        # Truncate if needed\n",
    "        if max_seq_len:\n",
    "            review_tokens_ids = review_tokens_ids[:max_seq_len]\n",
    "\n",
    "        for idx in range(len(review_tokens_ids)):\n",
    "            input_word = review_tokens_ids[idx]\n",
    "            \n",
    "            # Context window boundaries\n",
    "            start = max(idx - window_size, 0)\n",
    "            end   = min(idx + window_size + 1, len(review_tokens_ids))\n",
    "            \n",
    "            # Context words = all words in window except the input word itself\n",
    "            context_words = [review_tokens_ids[i] for i in range(start, end) if i != idx]\n",
    "            \n",
    "            # Append pairs\n",
    "            batch_input_word.extend([input_word] * len(context_words))\n",
    "            batch_target_words.extend(context_words)\n",
    "\n",
    "    # Convert to tensors\n",
    "    if not batch_input_word:\n",
    "        return torch.tensor([]), torch.tensor([])\n",
    "        \n",
    "    batch_input_word = torch.tensor(batch_input_word, dtype=torch.long)\n",
    "    batch_target_words = torch.tensor(batch_target_words, dtype=torch.long)\n",
    "    \n",
    "    return batch_input_word, batch_target_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fc1a5c58-b644-4fb4-9520-9b7e9f700f99",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'partial' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# --- Create DataLoaders using the collate function\u001b[39;00m\n\u001b[32m     20\u001b[39m batch_size = \u001b[32m128\u001b[39m\n\u001b[32m     21\u001b[39m traindl_skipgram = DataLoader(\n\u001b[32m     22\u001b[39m     train_iter,\n\u001b[32m     23\u001b[39m     batch_size=batch_size,\n\u001b[32m     24\u001b[39m     shuffle=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m     collate_fn=\u001b[43mpartial\u001b[49m(collate_skipgram, text_pipeline=text_pipeline, window_size=\u001b[32m4\u001b[39m)\n\u001b[32m     26\u001b[39m )\n\u001b[32m     28\u001b[39m evaldl_skipgram = DataLoader(\n\u001b[32m     29\u001b[39m     test_iter,\n\u001b[32m     30\u001b[39m     batch_size=batch_size,\n\u001b[32m     31\u001b[39m     shuffle=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     32\u001b[39m     collate_fn=partial(collate_skipgram, text_pipeline=text_pipeline, window_size=\u001b[32m4\u001b[39m)\n\u001b[32m     33\u001b[39m )\n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m# Test the DataLoader and model\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'partial' is not defined"
     ]
    }
   ],
   "source": [
    "# --- SkipGram model\n",
    "class SkipGram(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=300, max_norm=1):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(\n",
    "            num_embeddings=vocab_size,\n",
    "            embedding_dim=embed_dim,\n",
    "            max_norm=max_norm\n",
    "        )\n",
    "        self.linear = nn.Linear(\n",
    "            in_features=embed_dim,\n",
    "            out_features=vocab_size\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.embeddings(x)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "# --- Create DataLoaders using the collate function\n",
    "batch_size = 128\n",
    "traindl_skipgram = DataLoader(\n",
    "    train_iter,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=partial(collate_skipgram, text_pipeline=text_pipeline, window_size=4)\n",
    ")\n",
    "\n",
    "evaldl_skipgram = DataLoader(\n",
    "    test_iter,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=partial(collate_skipgram, text_pipeline=text_pipeline, window_size=4)\n",
    ")\n",
    "\n",
    "# Test the DataLoader and model\n",
    "vocab_size = len(vocab)\n",
    "model = SkipGram(vocab_size, embed_dim=300).to(device)\n",
    "\n",
    "for xb, yb in traindl_skipgram:\n",
    "    print(f\"Batch shapes: {xb.shape}, {yb.shape}\")\n",
    "    out = model(xb.to(device))\n",
    "    print(\"Model output shape:\", out.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46eb7f92-7af2-42c1-9fbe-abf82d45ffb8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (torch_env)",
   "language": "python",
   "name": "torch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
